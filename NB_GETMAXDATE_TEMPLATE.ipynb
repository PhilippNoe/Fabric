{"cells":[{"cell_type":"markdown","source":["Generalized Get Max Timestamp Notebook\n","=======================================\n","Retrieves maximum timestamp from any table/column configuration.\n","\n","Prerequsite: The lakehouse which holds the targeted table must be set as default lakehouse for this notebook\n","\n","Parameter: \n","- table_name: Table name (required)\n","- date_column: Date/datetime column name (required) \n","- time_column: Optional separate time column (default: None)\n","- output_format: Output format (default: \"%Y-%m-%d %H:%M:%S\")"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"70e5c7b8-bd98-46ae-b298-6c7e8c6f197f"},{"cell_type":"code","source":["# =============================================================================\n","# PARAMETERS\n","# =============================================================================\n","table_name = \"Facts\"\n","date_column = \"Changed\"\n","time_column = \"None\"\n","output_format = \"%Y-%m-%d %H:%M:%S\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":10,"statement_ids":[10],"state":"finished","livy_statement_state":"available","session_id":"6d7fc7d0-8d40-4554-9dd3-af3ae9925a37","normalized_state":"finished","queued_time":"2025-11-21T15:33:45.5760695Z","session_start_time":null,"execution_start_time":"2025-11-21T15:33:45.5771214Z","execution_finish_time":"2025-11-21T15:33:46.1041931Z","parent_msg_id":"a95bf640-0e18-4e56-a882-cf44aa298169"},"text/plain":"StatementMeta(, 6d7fc7d0-8d40-4554-9dd3-af3ae9925a37, 10, Finished, Available, Finished)"},"metadata":{}}],"execution_count":8,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"91918def-ee68-477b-a594-e3ddb995a1d6"},{"cell_type":"code","source":["# =============================================================================\n","# IMPORTS & SPARK CONFIG\n","# =============================================================================\n","from pyspark.sql import functions as F\n","from notebookutils import mssparkutils\n","from datetime import datetime, date\n","\n","# Parquet datetime handling\n","spark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInRead\", \"CORRECTED\")\n","spark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")\n","spark.conf.set(\"spark.sql.legacy.parquet.datetimeRebaseModeInRead\", \"CORRECTED\")\n","spark.conf.set(\"spark.sql.legacy.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","session_id":"6d7fc7d0-8d40-4554-9dd3-af3ae9925a37","normalized_state":"finished","queued_time":"2025-11-21T15:33:45.6468896Z","session_start_time":null,"execution_start_time":"2025-11-21T15:33:46.106209Z","execution_finish_time":"2025-11-21T15:33:46.5734641Z","parent_msg_id":"6c8b300c-d82b-4ddb-8ea0-c5d1b6ae6e8f"},"text/plain":"StatementMeta(, 6d7fc7d0-8d40-4554-9dd3-af3ae9925a37, 11, Finished, Available, Finished)"},"metadata":{}}],"execution_count":9,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0d6d6e76-da06-4a59-b8f9-e3e9f3e4c29e"},{"cell_type":"code","source":["# =============================================================================\n","# MAIN LOGIC\n","# =============================================================================\n","def get_max_timestamp(table_name, date_column, time_column, output_format):\n","\n","    # Validate inputs\n","    if not table_name or not date_column:\n","        raise ValueError(\"table_name and date_column are required\")\n","    \n","    # Load table\n","    try:\n","        df = spark.read.table(table_name)\n","    except Exception as e:\n","        raise RuntimeError(f\"Failed to load table '{table_name}': {str(e)}\")\n","    \n","    # Return empty string if table is empty\n","    if df.rdd.isEmpty():\n","        return \"\"\n","    \n","    # Process based on column configuration\n","    if time_column != \"None\":\n","        # Separate date + time columns\n","        if date_column not in df.columns or time_column not in df.columns:\n","            raise ValueError(\n","                f\"Required columns not found. \"\n","                f\"Needed: [{date_column}, {time_column}]. \"\n","                f\"Available: {df.columns}\"\n","            )\n","        \n","        max_ts = (\n","            df.withColumn(\n","                \"_ts\",\n","                F.to_timestamp(\n","                    F.concat_ws(\" \",\n","                        F.date_format(F.col(date_column), \"yyyy-MM-dd\"),\n","                        F.col(time_column).cast(\"string\")\n","                    ),\n","                    \"yyyy-MM-dd HH:mm:ss\"\n","                )\n","            )\n","            .select(F.max(\"_ts\"))\n","            .collect()[0][0]\n","        )\n","    else:\n","        # Single datetime/date column\n","        if date_column not in df.columns:\n","            raise ValueError(\n","                f\"Column '{date_column}' not found. \"\n","                f\"Available: {df.columns}\"\n","            )\n","        \n","        max_ts = df.select(F.max(date_column)).collect()[0][0]\n","    \n","    # Format and return\n","    if max_ts is None:\n","        return \"\"\n","    elif isinstance(max_ts, datetime):\n","        return max_ts.strftime(output_format)\n","    elif isinstance(max_ts, date):\n","        return datetime.combine(max_ts, datetime.min.time()).strftime(output_format)\n","    else:\n","        return str(max_ts)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":12,"statement_ids":[12],"state":"finished","livy_statement_state":"available","session_id":"6d7fc7d0-8d40-4554-9dd3-af3ae9925a37","normalized_state":"finished","queued_time":"2025-11-21T15:33:45.8121423Z","session_start_time":null,"execution_start_time":"2025-11-21T15:33:46.5754631Z","execution_finish_time":"2025-11-21T15:33:47.0059385Z","parent_msg_id":"5c0c7905-6cb5-4643-8d2a-1cbda88ed896"},"text/plain":"StatementMeta(, 6d7fc7d0-8d40-4554-9dd3-af3ae9925a37, 12, Finished, Available, Finished)"},"metadata":{}}],"execution_count":10,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"be0aba44-7abc-47fb-a3e3-b9286e8e36c3"},{"cell_type":"code","source":["# =============================================================================\n","# EXECUTION\n","# =============================================================================\n","latest_timestamp_str = get_max_timestamp(table_name, date_column, time_column, output_format)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":13,"statement_ids":[13],"state":"finished","livy_statement_state":"available","session_id":"6d7fc7d0-8d40-4554-9dd3-af3ae9925a37","normalized_state":"finished","queued_time":"2025-11-21T15:33:46.0498397Z","session_start_time":null,"execution_start_time":"2025-11-21T15:33:47.0076423Z","execution_finish_time":"2025-11-21T15:33:49.695556Z","parent_msg_id":"4300ad12-cfcf-4bb2-8707-e37a7cb18ea8"},"text/plain":"StatementMeta(, 6d7fc7d0-8d40-4554-9dd3-af3ae9925a37, 13, Finished, Available, Finished)"},"metadata":{}}],"execution_count":11,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"99320bc8-7570-4399-a1db-e6f5be4f13ef"},{"cell_type":"code","source":["# =============================================================================\n","# RETURN TO PIPELINE\n","# =============================================================================\n","mssparkutils.notebook.exit(latest_timestamp_str)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":14,"statement_ids":[14],"state":"finished","livy_statement_state":"available","session_id":"6d7fc7d0-8d40-4554-9dd3-af3ae9925a37","normalized_state":"finished","queued_time":"2025-11-21T15:33:46.264385Z","session_start_time":null,"execution_start_time":"2025-11-21T15:33:49.6973409Z","execution_finish_time":"2025-11-21T15:33:50.0480637Z","parent_msg_id":"23f7f021-d00f-4c67-9f39-423ec9fa37ce"},"text/plain":"StatementMeta(, 6d7fc7d0-8d40-4554-9dd3-af3ae9925a37, 14, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["ExitValue: 2025-11-10 07:26:11"]}],"execution_count":12,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5a1db90f-e191-4d2c-a8bc-04a779709fa2"},{"cell_type":"markdown","source":["### One cell version"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b638fbe1-5e59-4745-a19c-ab355b6032c5"},{"cell_type":"code","source":["# =============================================================================\n","# PARAMETERS\n","# =============================================================================\n","table_name = \"Facts\"\n","date_column = \"Changed\"\n","time_column = \"None\"\n","output_format = \"%Y-%m-%d %H:%M:%S\"\n","\n","# =============================================================================\n","# IMPORTS & SPARK CONFIG\n","# =============================================================================\n","from pyspark.sql import functions as F\n","from notebookutils import mssparkutils\n","from datetime import datetime, date\n","\n","# Parquet datetime handling\n","spark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInRead\", \"CORRECTED\")\n","spark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")\n","spark.conf.set(\"spark.sql.legacy.parquet.datetimeRebaseModeInRead\", \"CORRECTED\")\n","spark.conf.set(\"spark.sql.legacy.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")\n","\n","# =============================================================================\n","# MAIN LOGIC\n","# =============================================================================\n","def get_max_timestamp(table_name, date_column, time_column, output_format):\n","\n","    # Validate inputs\n","    if not table_name or not date_column:\n","        raise ValueError(\"table_name and date_column are required\")\n","    \n","    # Load table\n","    try:\n","        df = spark.read.table(table_name)\n","    except Exception as e:\n","        raise RuntimeError(f\"Failed to load table '{table_name}': {str(e)}\")\n","    \n","    # Return empty string if table is empty\n","    if df.rdd.isEmpty():\n","        return \"\"\n","    \n","    # Process based on column configuration\n","    if time_column != \"None\":\n","        # Separate date + time columns\n","        if date_column not in df.columns or time_column not in df.columns:\n","            raise ValueError(\n","                f\"Required columns not found. \"\n","                f\"Needed: [{date_column}, {time_column}]. \"\n","                f\"Available: {df.columns}\"\n","            )\n","        \n","        max_ts = (\n","            df.withColumn(\n","                \"_ts\",\n","                F.to_timestamp(\n","                    F.concat_ws(\" \",\n","                        F.date_format(F.col(date_column), \"yyyy-MM-dd\"),\n","                        F.col(time_column).cast(\"string\")\n","                    ),\n","                    \"yyyy-MM-dd HH:mm:ss\"\n","                )\n","            )\n","            .select(F.max(\"_ts\"))\n","            .collect()[0][0]\n","        )\n","    else:\n","        # Single datetime/date column\n","        if date_column not in df.columns:\n","            raise ValueError(\n","                f\"Column '{date_column}' not found. \"\n","                f\"Available: {df.columns}\"\n","            )\n","        \n","        max_ts = df.select(F.max(date_column)).collect()[0][0]\n","    \n","    # Format and return\n","    if max_ts is None:\n","        return \"\"\n","    elif isinstance(max_ts, datetime):\n","        return max_ts.strftime(output_format)\n","    elif isinstance(max_ts, date):\n","        return datetime.combine(max_ts, datetime.min.time()).strftime(output_format)\n","    else:\n","        return str(max_ts)\n","\n","# =============================================================================\n","# EXECUTION\n","# =============================================================================\n","latest_timestamp_str = get_max_timestamp(table_name, date_column, time_column, output_format)\n","\n","# =============================================================================\n","# RETURN TO PIPELINE\n","# =============================================================================\n","mssparkutils.notebook.exit(latest_timestamp_str)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":15,"statement_ids":[15],"state":"finished","livy_statement_state":"available","session_id":"6d7fc7d0-8d40-4554-9dd3-af3ae9925a37","normalized_state":"finished","queued_time":"2025-11-21T15:34:40.6722453Z","session_start_time":null,"execution_start_time":"2025-11-21T15:34:40.6735537Z","execution_finish_time":"2025-11-21T15:34:44.4386597Z","parent_msg_id":"ac170560-f531-424a-b305-ee4f42171e66"},"text/plain":"StatementMeta(, 6d7fc7d0-8d40-4554-9dd3-af3ae9925a37, 15, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["ExitValue: 2025-11-10 07:26:11"]}],"execution_count":13,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":false,"run_control":{"frozen":true}},"id":"1ce71523-365e-4a50-b296-9045b8d454d1"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"53e0a250-c86d-41b0-8b3c-7e8b9a0e730a"}],"default_lakehouse":"53e0a250-c86d-41b0-8b3c-7e8b9a0e730a","default_lakehouse_name":"LH_Bronze_PoC","default_lakehouse_workspace_id":"dd0d9683-1d7d-42cd-99d5-cab2032e1cbb"}}},"nbformat":4,"nbformat_minor":5}